/*Decision Trees

Key ideas

Iteratively split variables into groups
Evaluate "homogeneity" within each group
Split again if necessary

Pros:
Easy to interpret
Better performance in nonlinear settings

Cons:
Without pruning/cross-validation can lead to overfitting
Harder to estimate uncertainty
Results may be variable

Basic algorithm

Start with all variables in one group
Find the variable/split that best separates the outcomes
Divide the data into two groups ("leaves") on that split ("node")
Within each split, find the best variable/split that separates the outcomes
Continue until the groups are too small or sufficiently "pure"

*/
  
#Example: Iris Data

data(iris); library(ggplot2)
names(iris)
table(iris$Species)
Create training and test sets

inTrain <- createDataPartition(y=iris$Species,
                               p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
Iris petal widths/sepal width

qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
Iris petal widths/sepal width

library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)

#Plot tree

plot(modFit$finalModel, uniform=TRUE, 
     main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
Prettier plots

library(rattle)
fancyRpartPlot(modFit$finalModel)
Predicting new values

predict(modFit,newdata=testing)

#Notes and further resources

/*Classification trees are non-linear models
They use interactions between variables
Data transformations may be less important (monotone transformations)
Trees can also be used for regression problems (continuous outcome)
Note that there are multiple tree building options in R 
both in the caret package - party, rpart and out of the caret package - tree*/
  

##############################################################
##############################################################

/*Random forests

Bootstrap samples
At each split, bootstrap variables
Grow multiple trees and vote

Pros:
Accuracy

Cons:
Speed
Interpretability
Overfitting*/

#Iris data

data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
                               p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]

#Random forests

library(caret)
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
modFit

#Getting a single tree

getTree(modFit$finalModel,k=2)
Class "centers"

irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
Predicting new values

pred <- predict(modFit,testing); testing$predRight <- pred==testing$Species
table(pred,testing$Species)
Predicting new values

qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="newdata Predictions")

#Notes and further resources
#Notes:
  
/*Random forests are usually one of the two top performing algorithms along with boosting in prediction contests.
Random forests are difficult to interpret but often very accurate.
Care should be taken to avoid overfitting (see rfcv funtion)*/
