######## K means clustering #########
######## Hierarchical clustering #########

determining the number of clusters in the data set
1. the elbow method: the percentage of variance explained as a function of the number of clusters.
if one plots the percentage of variance explained by the clusters against the number of clusters, 
the first clusters will add much information (explain a lot of variance), 
but at some point the marginal gain will drop, giving an angle in the graph. 
The number of clusters is chosen at this point, hence the "elbow criterion"

2. Information Criterion Approach
AIC, BIC

3. Cross Validation
the data is partitioned into v parts, each of the parts is taking turns to be set aside as the testing
data. A clustering model is trained with other v-1 datasets, and the value of the goal function (
  can be the sum of squared distance to the centroids for k means) to be calculated for each number
of clusters, pick the one who miniimzed the test errors.


#How to define close?
distance or similarity
-continuous, euclidean distance
-continuous, correlation similarity
-binary, manhattan distance

#A partioning approach
- Fix a number of clusters
- Get"centroids"of each cluster
- Assign things to closest centroid 
- Reclaculate centroids

#Requires
- A defined distance metric
- A number of clusters
- An initial guessas to cluster centroids

#Produces
- Final estimate of cluster centroids
- An assignment of each point to clusters

#Example
set.seed(123)
par(mar=c(0,0,0,0))
x<-rnorm(12,mean=rep(1:3, each = 4), sd=0.2)
y<-rnorm(12,mean=rep(c(1,2,1),each = 4), sd=0.2)

plot(x,y,col="blue",pch=19, cex = 2)
text(x+0.05,y+0.05, labels=as.character(1:12))

dataFrame<-data.frame(x,y)
kmeansobj<-kmeans(dataFrame,centers=3)
names(kmeansobj)

par(mar=rep(0.2,4))
plot(x,y,col=kmeansobj$cluster,pch=19,cex=2)
points(kmeansobj$centers,col=1:3,pch=3,cex=3,lwd=3)


######## Hierarchical clustering #########

An agglomerative approach - Findclosesttwothings - Putthemtogether
- Findnextclosest
· Requires
- Adefineddistance
- Amergingapproach
· Produces
- Atreeshowinghowclosethingsaretoeachother



dist(dataFrame)
dataFrame<- data.frame(x=x,y=y)
distxy<- dist(dataFrame)
hClustering<-hclust(distxy)
plot(hClustering)
